<!DOCTYPE html>
<html lang="en">
  <title>Oliver Thomas Thesis</title>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <script src="custom_themes/html_elements.js"></script>
  <link rel="stylesheet" href="dist/reveal.css">
  <link rel="stylesheet" href="custom_themes/sussex2.css" id="theme">

  <div class="reveal">

    <div class="slides">

      <div id="background-template">
        <footer>
          <p>Thesis Overview
        </footer>
      </div>

      <section class="dark-cyan">
        <p>&nbsp;
        <h2>Thesis Overview</h2>
        <p>&nbsp;
        <h3>Oliver Thomas</h3>
        <p>&nbsp;
        <p style="color: white">Predictive Analytics Lab<br>
          University of Sussex
        <p><img src="images/logos/University_of_Sussex_Logo.svg2000_white.png" style="width: 5rem;">
      </section>

      <section>
        <h2>Contents</h2>
        <ol type="1">
          <li>Background
            <ol style="list-style-type: lower-alpha">
              <li>Motivation</li>
              <li>Fair Machine Learning &amp; Fair Representations</li>
            </ol>
          </li>
          <li>Chapters
            <ol style="list-style-type: lower-alpha">
              <li>A Novel Use-case for Fair Representations</li>
              <li>Fair Representations in the Data Domain</li>
              <li>One Step Further</li>
            </ol>
          </li>
          <li>Conclusion</li>
        </ol>
      </section>

      <section class="dark-cyan">
        <p>&nbsp;
        <p>&nbsp;
        <p>&nbsp;
        <p>&nbsp;
        <h2>Background</h2>
      </section>

      <section>
      <h3>Motivation</h3>
      <aside class="notes">
        Machine Learning is a tool that is growing in popularity. There have been a number of high profile successes, and new applications are being regularly identified. These applications include translation (in both image and natural-language domains), pattern recognition and decision-making. Contexts for these applications include Geology, Meteorology, Sports forecasting and Agriculture, to name a few.

        Because of this success there is a desire to incorporate these systems in more and more situations, including those directly applicable to people. For example, Machine Learning systems have already been applied to police allocation, recidivism prediction, candidate screening and credit approval.

        On top of the benefits of automated decision making (speed, scale, etc) there is an additional promise to automated decisions. The promise is that instead of many human decision makers, each one biased with their own prejudices, heuristics and experience, we can have a uniform approach. The hope is that by treating everybody the same, then unequal, biased behaviour can be removed.

        Unfortunately, that's not always the case.

        Recent headlines include:

        Wrongfully Accused by an Algorithm: In what may be the first known case of its kind, a faulty facial recognition match led to a Michigan man’s arrest for a crime he did not commit. -- NYTimes {cite}Hill20
        Wrongfully Accused by an Algorithm. -- The Guardian {cite}Reu18
        Police officers raise concerns about 'biased' AI data. -- BBC News {cite}BBC19
        But how does this happen? A prediction model has to be designed and there are a number of legal and moral obstacles to prevent a group/individual from purposefully designing a biased system. However, even for the best intentioned there are a number of potential problems. Examples of these problems include (but are not limited to):
      </aside>
        <p></p>
        <div class="r-stack" style="padding-top: 2.5%">
          <img src="images/amazon.png" width="50%">
          <img class="fragment" src="images/pp_mb.png" width="50%" style="transform:rotate(-5deg);">
          <img class="fragment" src="images/wapo.png" width="50%" style="transform:rotate(5deg);">
        </div>
      </section>
      <section>
        <section>
          <h3>Motivation</h3>
          <h4>Why does this happen?</h4>
          <div style="text-align: left; padding-left: 5%">
            <p>Some reasons include:</p>
            <ul>
              <li><strong>Systematic biases:</strong> Unfair practices are encoded in "upstream" systems.</li>
              <li><strong>Statistical biases:</strong> We don't have representative data of our population.</li>
              <li><strong>Proxy Labels:</strong> We don't (or can't) measure what we truly want to measure, so use a related quality as a proxy.</li>
              <li><strong>Underspecified Objective</strong>: The optimisation target is misjudged.</li>
              <li>... and many more</li>
            </ul>
          </div>
<!--          <p>An unconstrained machine learning model is susceptible to all of these problems. To face this challenge, the machine learning community has focused on creating a class of learning models that are constrained to exhibit less bias than an unconstrained model. Typically, these are referred to as “Fair Machine Learning Models”.</p>-->
        </section>
        <section>
          <img src="images/data_disparities.png" width="55%">
        </section>
      </section>

      <section>
        <h3>Background</h3>
        <multi-col>
          <one-col>
            <h4>Fair Machine Learning</h4>
            <ul>
              <li>Typically defined in terms of classification e.g.</li>
              <ul>
                <li>$f(x) \mapsto y \quad\mathrm{s.t.}\;\,y \perp s$</li>
              </ul>
              <li>Notions of fairness.
                <ul>
                  <li>Group Fairness: Similar performance across demographic groups</li>
                  <li>Individual Fairness: Similar individuals should be treated similarly</li>
                </ul>
              </li>
            </ul>
          </one-col>
          <one-col>
            <h4>Fair Representations</h4>
            <ul>
              <li>Remains defined in terms of classification, but invariance extended to intermediate step.</li>
              <ul>
                <li>$f(x) \mapsto z \quad\mathrm{s.t.}\;\,z \perp s$</li>
                <li>$g(z) \mapsto y \quad\mathrm{s.t.}\;\,y \perp s$</li>
              </ul>
              <li>Reconstructing $x$ from $z$ allows for unsupervised training.</li>
              <li>Can improve fairness measures for multiple targets.</li>
            </ul>
          </one-col>
        </multi-col>
      </section>


      <section class="dark-cyan">
        <p>&nbsp;
        <p>&nbsp;
        <p>&nbsp;
        <p>&nbsp;
        <h2>Chapters</h2>
      </section>

      <section>
        <h3>A Novel Use-case for Fair Representations</h3>

        <section>
          <small>Thomas, Oliver, Miri Zilka, Adrian Weller and Novi Quadrianto.</small>
          <small>‘An Algorithmic Framework for Positive Action’.</small>
          <small>Conference: Equity and Access in Algorithms, Mechanisms, and Optimization. EAAMO ’21.</small>
          <h4>Reconstructing based on Fair Representations</h4>
          <p>"Similar individals should be treated similarly"</p>
          <multi-col>
            <one-col><img src="images/paf_1.png"></one-col>
            <one-col><img src="images/paf_2.png"></one-col>
            <one-col><img src="images/paf_3.png"></one-col>
          </multi-col>
          <p>If $z$ is independent of $s$, let's reconstruct with other $s$ values.</p>
        </section>
        <section>
          <h4>Positive Action Apporach</h4>
          <p>Alternative reconstructions are interesting, but what do they tell us?</p>
          <p>&nbsp;</p>
          <p>Limited counterfactual question: How would someone have changed if a fundamental attribute such as  $s$ were different?</p>
          <p>Connection to individual fairness, but ultimately about parity.</p>
          <p>Positive Action suggested as a remedy for those affected.</p>
        </section>
        <section>
          <h4>Results</h4>
          <multi-col>
            <one-col>
              <p><img src="images/paf_groups.png"></p>
            </one-col>
            <one-col>
              <p><img src="images/paf_table.png"></p>
            </one-col>
          </multi-col>
        </section>
        <section>
          <img src="images/paf_model.png" width="50%">
        </section>
      </section>

      <section>
        <h3>Fair Representations in the Data Domain</h3>

        <section>
          <small>Quadrianto, Novi, Viktoriia Sharmanska and Oliver Thomas.</small>
          <small>‘Discovering Fair Representations in the Data Domain’.</small>
          <small>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) '19.</small>
          <h4>Something Else?</h4>
          <multi-col>
            <one-col><img src="images/dfrdd_0.png"></one-col>
            <one-col><img src="images/dfrdd_1.png"></one-col>
            <one-col><img src="images/dfrdd_2.png"></one-col>
          </multi-col>
          <p>Instead of reconstructing based on $s$, can we map to somewhere inbetween?</p>
          <p>A fair representation of the data, that stays in the same domain as the data.</p>
        </section>

        <section>
          <h4>How?</h4>
          <p>Decomposition Assumption:</p>
          <p>Original input = Fair component + Unfair component</p>
          <p>&nbsp;</p>
          <p>Constraints:
          <ol>
            <li>Fair component must be similar to the original.</li>
            <li>Fair component must be predictive of $y$.</li>
            <li>Fair component must be independent of $s$.</li>
            <li>Unfair component can contain information related to $s$.</li>
          </ol>
          </p>
        </section>

        <section>
          <h4>Results</h4>
          <img src="images/dfrdd_table_adult.png" width="40%">
          <multi-col>
            <one-col><img src="images/dfrdd_images_0.png"></one-col>
            <one-col><img src="images/dfrdd_images.png"></one-col>
          </multi-col>
        </section>
        
        <section>
          <img src="images/architecture.png">
        </section>
        
        <section>
          <h4>Addendum</h4>
          <p>Are there some improvements that we can make? Yes.</p>
          <ol>
            <li>Remove $y$-loss term.</li>
            <li>Adversary and Reconstruction term operate on range of residuals.</li>
          </ol>
          <multi-col>
            <one-col>
              <p><img src="images/dfrdd_addendum_cmnist.png" width="100%"></p>
            </one-col>
            <one-col>
              <p>&nbsp;</p>
              <p>An improvement, but still limited due to problem formulation...</p>
              <p>Maybe a different approach is needed?</p>
            </one-col>
          </multi-col>
        </section>
      </section>

      <section>
        <h3>One Step Further</h3>

        <section>
          <small>Kehrenberg, Thomas, Myles Bartlett, Oliver Thomas and Novi Quadrianto.</small>
          <small>‘Null-Sampling for Interpretable and Fair Representations’.</small>
          <small>European Conference on Computer Vision (ECCV) '20.</small>
          <h4>Invertibility</h4>
          <multi-col>
            <one-col><img src="images/nifr_0.png"></one-col>
            <one-col><img src="images/nifr_1.png"></one-col>
          </multi-col>
          <p>Use the <em>invertible</em> property of Normalizing Flow models to solve previous issues.</p>
        </section>
        <section>
          <img src="images/nifr_arch.png">
        </section>
        <section>
          <h4>Results</h4>
          <multi-col>
            <one-col><p><img src="images/nifr_multilabel.png"></p></one-col>
            <one-col><p><img src="images/nifr_images.png"></p></one-col>
          </multi-col>
        </section>
      </section>

      <section class="dark-cyan">
        <p>&nbsp;
        <p>&nbsp;
        <p>&nbsp;
        <h2>Conclusion &amp;</h2>
        <h2>Future Works</h2>
      </section>

    </div>
  </div>

  <script type="module" src="setup.js"></script>
